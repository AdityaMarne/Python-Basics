{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2cf66e4",
   "metadata": {},
   "source": [
    "1. **Explain the Activation Functions in your own language:**\n",
    "   - **Sigmoid**: The sigmoid function squashes input values to a range between 0 and 1. It's useful for binary classification problems because it outputs probabilities. However, it can suffer from vanishing gradients, making training deep networks difficult.\n",
    "     $$\\sigma(x) = \\frac{1}{1 + e^{-x}}$$\n",
    "\n",
    "   - **Tanh**: The tanh function squashes input values to a range between -1 and 1. It's zero-centered, which can help with convergence during training. Like the sigmoid, it can also suffer from vanishing gradients.\n",
    "     $$\\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}$$\n",
    "\n",
    "   - **ReLU (Rectified Linear Unit)**: ReLU outputs the input directly if it is positive; otherwise, it outputs zero. It's simple and effective, helping to mitigate the vanishing gradient problem. However, it can suffer from the \"dying ReLU\" problem, where neurons can get stuck outputting zero.\n",
    "     $$\\text{ReLU}(x) = \\max(0, x)$$\n",
    "\n",
    "   - **ELU (Exponential Linear Unit)**: ELU outputs the input if it is positive, and an exponential function if it is negative. This helps to keep the mean activations closer to zero and can improve learning dynamics.\n",
    "     $$\\text{ELU}(x) = \\begin{cases} \n",
    "      x & \\text{if } x > 0 \\\\\n",
    "      \\alpha (e^x - 1) & \\text{if } x \\leq 0 \n",
    "   \\end{cases}$$\n",
    "\n",
    "   - **Leaky ReLU**: Leaky ReLU is a variant of ReLU that allows a small, non-zero gradient when the input is negative. This helps to mitigate the dying ReLU problem.\n",
    "     $$\\text{Leaky ReLU}(x) = \\begin{cases} \n",
    "      x & \\text{if } x > 0 \\\\\n",
    "      \\alpha x & \\text{if } x \\leq 0 \n",
    "   \\end{cases}$$\n",
    "\n",
    "   - **Swish**: Swish is a newer activation function that is a smooth, non-monotonic function. It tends to perform better than ReLU in deeper networks.\n",
    "     $$\\text{Swish}(x) = x \\cdot \\sigma(x) = x \\cdot \\frac{1}{1 + e^{-x}}$$\n",
    "\n",
    "2. **What happens when you increase or decrease the optimizer learning rate?**\n",
    "   - **Increase**: Increasing the learning rate can speed up training, but if it's too high, it can cause the model to overshoot the optimal solution, leading to divergence or instability.\n",
    "   - **Decrease**: Decreasing the learning rate can lead to more stable training and convergence, but if it's too low, training can become very slow and may get stuck in local minima.\n",
    "\n",
    "3. **What happens when you increase the number of internal hidden neurons?**\n",
    "   - Increasing the number of hidden neurons can allow the network to learn more complex patterns and improve performance. However, it also increases the risk of overfitting, especially if the training data is limited. It also increases computational cost and memory usage.\n",
    "\n",
    "4. **What happens when you increase the size of batch computation?**\n",
    "   - Increasing the batch size can lead to more stable gradient estimates and faster training due to better utilization of hardware. However, if the batch size is too large, it can require more memory and may lead to poorer generalization.\n",
    "\n",
    "5. **Why do we adopt regularization to avoid overfitting?**\n",
    "   - Regularization techniques, such as L1/L2 regularization, dropout, and early stopping, help to prevent overfitting by adding constraints or noise to the model. This encourages the model to learn more general patterns rather than memorizing the training data.\n",
    "\n",
    "6. **What are loss and cost functions in deep learning?**\n",
    "   - **Loss Function**: A loss function measures the difference between the predicted output and the actual target. It is used to guide the optimization process.\n",
    "   - **Cost Function**: The cost function is typically the average of the loss function over the entire training dataset. It provides a single scalar value that the optimization algorithm aims to minimize.\n",
    "\n",
    "7. **What do you mean by underfitting in neural networks?**\n",
    "   - Underfitting occurs when a neural network is too simple to capture the underlying patterns in the data. This results in poor performance on both the training and validation datasets. It can be caused by having too few neurons, layers, or insufficient training.\n",
    "\n",
    "8. **Why do we use Dropout in Neural Networks?**\n",
    "   - Dropout is used to prevent overfitting by randomly setting a fraction of the neurons to zero during training. This forces the network to learn redundant representations and improves generalization by reducing reliance on any single neuron.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2edaf7",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
