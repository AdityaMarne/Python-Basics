{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "306ae318",
   "metadata": {},
   "source": [
    "\n",
    "1. **Is it OK to initialize all the weights to the same value as long as that value is selected randomly using He initialization?**\n",
    "   No, it is not OK to initialize all the weights to the same value, even if that value is selected randomly using He initialization. Initializing all weights to the same value would result in symmetry, causing all neurons to learn the same features during training. He initialization involves setting weights to small random values drawn from a specific distribution to break this symmetry.\n",
    "\n",
    "2. **Is it OK to initialize the bias terms to 0?**\n",
    "   Yes, it is generally OK to initialize the bias terms to 0. Bias terms are not subject to the same symmetry issues as weights, so initializing them to zero does not prevent the network from learning effectively.\n",
    "\n",
    "3. **Three advantages of the SELU activation function over ReLU:**\n",
    "   - **Self-Normalizing**: SELU induces self-normalization, meaning the activations automatically converge to zero mean and unit variance, which helps in stabilizing the training process.\n",
    "   - **No Dead Neurons**: Unlike ReLU, SELU does not suffer from the \"dying ReLU\" problem where neurons can get stuck and stop learning.\n",
    "   - **Faster Learning**: SELU networks tend to learn faster and achieve better performance without the need for additional normalization techniques like batch normalization.\n",
    "\n",
    "4. **When to use each activation function:**\n",
    "   - **SELU**: Use SELU when you want self-normalizing properties, especially in deep networks without batch normalization.\n",
    "   - **Leaky ReLU (and variants)**: Use when you want to avoid the dying ReLU problem and need a small gradient for negative inputs.\n",
    "   - **ReLU**: Use for general purposes, especially in hidden layers of deep networks due to its simplicity and effectiveness.\n",
    "   - **Tanh**: Use when you need zero-centered outputs, which can help in faster convergence.\n",
    "   - **Logistic (Sigmoid)**: Use in the output layer for binary classification problems.\n",
    "   - **Softmax**: Use in the output layer for multi-class classification problems to get probability distributions over classes.\n",
    "\n",
    "5. **What may happen if you set the momentum hyperparameter too close to 1 (e.g., 0.99999) when using an SGD optimizer?**\n",
    "   Setting the momentum hyperparameter too close to 1 can cause the optimizer to overshoot the minimum, leading to instability and oscillations in the training process. It can also slow down convergence as the optimizer may take longer to settle into the minimum.\n",
    "\n",
    "6. **Three ways to produce a sparse model:**\n",
    "   - **Pruning**: Remove weights that are close to zero after training.\n",
    "   - **Regularization**: Use L1 regularization to encourage sparsity in the weights.\n",
    "   - **Sparse Initialization**: Start with a sparse network by initializing many weights to zero[^20^].\n",
    "\n",
    "7. **Does dropout slow down training? Does it slow down inference? What about MC Dropout?**\n",
    "   - **Training**: Dropout can slow down training because it requires additional computations to randomly drop units during each training iteration.\n",
    "   - **Inference**: Dropout does not slow down inference because it is typically turned off during this phase. However, MC Dropout, which applies dropout during inference to estimate uncertainty, can slow down inference as it requires multiple forward passes.\n",
    "\n",
    "8. **Practice training a deep neural network on the CIFAR10 image dataset:**\n",
    "   - **a. Build a DNN with 20 hidden layers of 100 neurons each using He initialization and the ELU activation function.**\n",
    "     ```python\n",
    "     import tensorflow as tf\n",
    "     from tensorflow.keras.layers import Dense, Flatten, ELU\n",
    "     from tensorflow.keras.models import Sequential\n",
    "\n",
    "     model = Sequential()\n",
    "     model.add(Flatten(input_shape=(32, 32, 3)))\n",
    "     for _ in range(20):\n",
    "         model.add(Dense(100, kernel_initializer='he_normal'))\n",
    "         model.add(ELU())\n",
    "     model.add(Dense(10, activation='softmax'))\n",
    "     ```\n",
    "\n",
    "   - **b. Using Nadam optimization and early stopping, train the network on the CIFAR10 dataset.**\n",
    "     ```python\n",
    "     from tensorflow.keras.datasets import cifar10\n",
    "     from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "     (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "     x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "     model.compile(optimizer='nadam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "     early_stopping = EarlyStopping(patience=10, restore_best_weights=True)\n",
    "     model.fit(x_train, y_train, epochs=100, validation_split=0.2, callbacks=[early_stopping])\n",
    "     ```\n",
    "\n",
    "   - **c. Add Batch Normalization and compare the learning curves.**\n",
    "     ```python\n",
    "     from tensorflow.keras.layers import BatchNormalization\n",
    "\n",
    "     model = Sequential()\n",
    "     model.add(Flatten(input_shape=(32, 32, 3)))\n",
    "     for _ in range(20):\n",
    "         model.add(Dense(100, kernel_initializer='he_normal'))\n",
    "         model.add(BatchNormalization())\n",
    "         model.add(ELU())\n",
    "     model.add(Dense(10, activation='softmax'))\n",
    "     model.compile(optimizer='nadam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "     model.fit(x_train, y_train, epochs=100, validation_split=0.2, callbacks=[early_stopping])\n",
    "     ```\n",
    "\n",
    "   - **d. Replace Batch Normalization with SELU and make necessary adjustments.**\n",
    "     ```python\n",
    "     model = Sequential()\n",
    "     model.add(Flatten(input_shape=(32, 32, 3)))\n",
    "     for _ in range(20):\n",
    "         model.add(Dense(100, kernel_initializer='lecun_normal'))\n",
    "         model.add(ELU())\n",
    "     model.add(Dense(10, activation='softmax'))\n",
    "     model.compile(optimizer='nadam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "     model.fit(x_train, y_train, epochs=100, validation_split=0.2, callbacks=[early_stopping])\n",
    "     ```\n",
    "\n",
    "   - **e. Regularize the model with alpha dropout and use MC Dropout for better accuracy.**\n",
    "     ```python\n",
    "     from tensorflow.keras.layers import AlphaDropout\n",
    "\n",
    "     model = Sequential()\n",
    "     model.add(Flatten(input_shape=(32, 32, 3)))\n",
    "     for _ in range(20):\n",
    "         model.add(Dense(100, kernel_initializer='lecun_normal'))\n",
    "         model.add(ELU())\n",
    "         model.add(AlphaDropout(0.1))\n",
    "     model.add(Dense(10, activation='softmax'))\n",
    "     model.compile(optimizer='nadam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "     model.fit(x_train, y_train, epochs=100, validation_split=0.2, callbacks=[early_stopping])\n",
    "\n",
    "     # MC Dropout during inference\n",
    "     import numpy as np\n",
    "\n",
    "     def mc_dropout_predict(model, x, n_iter=100):\n",
    "         f = tf.keras.backend.function([model.input, tf.keras.backend.learning_phase()], [model.output])\n",
    "         result = np.zeros((n_iter,) + model.output_shape)\n",
    "         for i in range(n_iter):\n",
    "             result[i] = f([x, 1])[0]\n",
    "         return result.mean(axis=0), result.std(axis=0)\n",
    "\n",
    "     mean, std = mc_dropout_predict(model, x_test)\n",
    "     ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c32bce5",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
