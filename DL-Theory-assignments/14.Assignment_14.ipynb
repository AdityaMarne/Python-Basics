{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cdab44bf",
   "metadata": {},
   "source": [
    "\n",
    "1. **Is it okay to initialize all the weights to the same value as long as that value is selected randomly using He initialization?**\n",
    "   - No, it is not okay to initialize all the weights to the same value, even if that value is selected randomly using He initialization. Initializing all weights to the same value would lead to symmetry problems, where all neurons in a layer would learn the same features and update in the same way during training. This would prevent the network from learning effectively.\n",
    "\n",
    "2. **Is it okay to initialize the bias terms to 0?**\n",
    "   - Yes, it is generally okay to initialize the bias terms to 0. Bias terms are added after the weights are applied, so initializing them to 0 does not cause the same symmetry issues as initializing weights to the same value.\n",
    "\n",
    "3. **Name three advantages of the ELU activation function over ReLU.**\n",
    "   - **Smoothness**: ELU is smoother than ReLU, which can lead to better convergence during training.\n",
    "   - **Negative Values**: ELU can output negative values, which helps the network to push mean activations closer to zero, improving the learning dynamics.\n",
    "   - **Less Likely to Die**: ELU reduces the likelihood of neurons dying (i.e., outputting zero for all inputs), a problem that can occur with ReLU.\n",
    "\n",
    "4. **In which cases would you want to use each of the following activation functions: ELU, leaky ReLU (and its variants), ReLU, tanh, logistic, and softmax?**\n",
    "   - **ELU**: Use when you need smooth and non-zero mean activations, especially in deep networks.\n",
    "   - **Leaky ReLU (and its variants)**: Use when you want to avoid the dying ReLU problem, where neurons output zero for all inputs.\n",
    "   - **ReLU**: Use as a default activation function due to its simplicity and effectiveness in many scenarios.\n",
    "   - **Tanh**: Use when you need outputs in the range \\([-1, 1]\\) and want zero-centered activations.\n",
    "   - **Logistic (Sigmoid)**: Use in the output layer for binary classification problems.\n",
    "   - **Softmax**: Use in the output layer for multi-class classification problems to obtain probability distributions over classes.\n",
    "\n",
    "5. **What may happen if you set the momentum hyperparameter too close to 1 (e.g., 0.99999) when using a MomentumOptimizer?**\n",
    "   - Setting the momentum hyperparameter too close to 1 can lead to very large updates, causing the optimization process to become unstable. The optimizer may overshoot the minimum, leading to divergence rather than convergence.\n",
    "\n",
    "6. **Name three ways you can produce a sparse model.**\n",
    "   - **L1 Regularization**: Applying L1 regularization encourages weights to become zero, leading to sparsity.\n",
    "   - **Pruning**: Removing weights that are below a certain threshold after training.\n",
    "   - **Sparse Initialization**: Initializing weights with a sparse matrix, where many weights are set to zero from the beginning.\n",
    "\n",
    "7. **Does dropout slow down training? Does it slow down inference (i.e., making predictions on new instances)?**\n",
    "   - **Training**: Dropout can slow down training because it requires additional computation to randomly drop units and scale the remaining activations.\n",
    "   - **Inference**: Dropout does not slow down inference because it is typically turned off during inference. Instead, the weights are scaled to account for the dropped units during training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b81621",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
