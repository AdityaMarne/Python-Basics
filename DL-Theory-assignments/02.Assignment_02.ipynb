{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7c4d403",
   "metadata": {},
   "source": [
    "\n",
    "1. **Structure of an Artificial Neuron**:\n",
    "   An artificial neuron mimics the function of a biological neuron. It consists of:\n",
    "   - **Inputs**: Analogous to dendrites, these receive signals.\n",
    "   - **Weights**: Each input is multiplied by a weight, similar to synaptic strength.\n",
    "   - **Summation Function**: Adds up all the weighted inputs.\n",
    "   - **Activation Function**: Determines if the neuron should fire, similar to the action potential in biological neurons.\n",
    "   - **Output**: The result of the activation function, analogous to the axon sending a signal to other neurons¹.\n",
    "\n",
    "2. **Types of Activation Functions**:\n",
    "   - **Sigmoid**: Outputs values between 0 and 1. Useful for binary classification but can suffer from vanishing gradients.\n",
    "   - **Tanh (Hyperbolic Tangent)**: Outputs values between -1 and 1. Zero-centered, which can help with training.\n",
    "   - **ReLU (Rectified Linear Unit)**: Outputs the input directly if positive; otherwise, it outputs zero. Helps mitigate the vanishing gradient problem.\n",
    "   - **Leaky ReLU**: Similar to ReLU but allows a small, non-zero gradient when the input is negative.\n",
    "   - **Softmax**: Converts a vector of values into a probability distribution. Useful for multi-class classification.\n",
    "\n",
    "3. **Rosenblatt’s Perceptron Model**:\n",
    "   - **Explanation**: The perceptron is a binary classifier that maps input features to an output using a linear function. It adjusts weights based on the error of the prediction.\n",
    "   - **Classification Example**:\n",
    "     - Weights: \\( w_0 = -1 \\), \\( w_1 = 2 \\), \\( w_2 = 1 \\)\n",
    "     - Data Points:\n",
    "       - (3, 4): \\( -1 + 2*3 + 1*4 = 9 \\) (Classified as 1)\n",
    "       - (5, 2): \\( -1 + 2*5 + 1*2 = 11 \\) (Classified as 1)\n",
    "       - (1, -3): \\( -1 + 2*1 + 1*(-3) = -2 \\) (Classified as 0)\n",
    "       - (-8, -3): \\( -1 + 2*(-8) + 1*(-3) = -20 \\) (Classified as 0)\n",
    "       - (-3, 0): \\( -1 + 2*(-3) + 1*0 = -7 \\) (Classified as 0)\n",
    "\n",
    "4. **Multi-Layer Perceptron (MLP)**:\n",
    "   - **Structure**: Consists of an input layer, one or more hidden layers, and an output layer. Each layer is fully connected to the next.\n",
    "   - **Solving XOR**: The hidden layer allows the network to learn non-linear decision boundaries, enabling it to solve problems like XOR that are not linearly separable.\n",
    "\n",
    "5. **Artificial Neural Network (ANN)**:\n",
    "   - **Definition**: A computational model inspired by the human brain, consisting of interconnected neurons.\n",
    "   - **Architectural Options**:\n",
    "     - **Feedforward Networks**: Signals flow in one direction.\n",
    "     - **Recurrent Networks**: Connections form cycles, allowing for memory.\n",
    "     - **Convolutional Networks**: Specialized for processing grid-like data, such as images.\n",
    "\n",
    "6. **Learning Process of an ANN**:\n",
    "   - **Process**: Involves adjusting weights based on the error of the output using algorithms like backpropagation.\n",
    "   - **Challenge**: Assigning synaptic weights can be difficult due to the complexity of the network. This is addressed using optimization algorithms like gradient descent.\n",
    "\n",
    "7. **Backpropagation Algorithm**:\n",
    "   - **Steps**:\n",
    "     - Forward pass: Compute the output.\n",
    "     - Compute the error.\n",
    "     - Backward pass: Calculate the gradient of the error with respect to each weight.\n",
    "     - Update weights using the gradient.\n",
    "   - **Limitations**: Can be slow and prone to getting stuck in local minima.\n",
    "\n",
    "8. **Adjusting Interconnection Weights**:\n",
    "   - Weights are adjusted based on the gradient of the error with respect to each weight. This is done iteratively until the error is minimized.\n",
    "\n",
    "9. **Steps in Backpropagation Algorithm**:\n",
    "   - Initialize weights.\n",
    "   - Forward pass.\n",
    "   - Compute error.\n",
    "   - Backward pass.\n",
    "   - Update weights.\n",
    "   - Repeat until convergence.\n",
    "   - **Need for Multi-Layer Networks**: Single-layer networks cannot solve non-linear problems. Multi-layer networks can learn complex patterns.\n",
    "\n",
    "10. **Short Notes**:\n",
    "    - **Artificial Neuron**: Basic unit of an ANN, mimicking a biological neuron.\n",
    "    - **Multi-Layer Perceptron**: A type of ANN with multiple layers, capable of learning non-linear patterns.\n",
    "    - **Deep Learning**: A subset of machine learning involving neural networks with many layers.\n",
    "    - **Learning Rate**: A hyperparameter that controls how much to change the model in response to the estimated error each time the model weights are updated.\n",
    "\n",
    "11. **Differences**:\n",
    "    - **Activation Function vs Threshold Function**: Activation functions can be non-linear and continuous, while threshold functions are typically binary.\n",
    "    - **Step Function vs Sigmoid Function**: Step function outputs binary values, while sigmoid outputs values between 0 and 1.\n",
    "    - **Single Layer vs Multi-Layer Perceptron**: Single-layer perceptrons can only solve linearly separable problems, while multi-layer perceptrons can solve complex, non-linear problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d594ace0",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
