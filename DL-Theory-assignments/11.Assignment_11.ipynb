{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0253a033",
   "metadata": {},
   "source": [
    "### 1. Python Code to Implement a Single Neuron\n",
    "Here's a simple implementation of a single neuron using the sigmoid activation function:\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "class Neuron:\n",
    "    def __init__(self, weights, bias):\n",
    "        self.weights = weights\n",
    "        self.bias = bias\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        return sigmoid(np.dot(inputs, self.weights) + self.bias)\n",
    "\n",
    "# Example usage\n",
    "weights = np.array([0.5, -0.6])\n",
    "bias = 0.1\n",
    "neuron = Neuron(weights, bias)\n",
    "inputs = np.array([1, 2])\n",
    "output = neuron.forward(inputs)\n",
    "print(output)\n",
    "```\n",
    "\n",
    "### 2. Python Code to Implement ReLU\n",
    "Here's how you can implement the ReLU activation function:\n",
    "```python\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "# Example usage\n",
    "x = np.array([-1, 2, -0.5, 3])\n",
    "output = relu(x)\n",
    "print(output)\n",
    "```\n",
    "\n",
    "### 3. Python Code for a Dense Layer in Terms of Matrix Multiplication\n",
    "Here's a dense layer implemented using matrix multiplication:\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "class DenseLayer:\n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.weights = np.random.randn(input_size, output_size)\n",
    "        self.bias = np.random.randn(output_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        return np.dot(inputs, self.weights) + self.bias\n",
    "\n",
    "# Example usage\n",
    "layer = DenseLayer(3, 2)\n",
    "inputs = np.array([[1, 2, 3]])\n",
    "output = layer.forward(inputs)\n",
    "print(output)\n",
    "```\n",
    "\n",
    "### 4. Python Code for a Dense Layer in Plain Python\n",
    "Here's a dense layer implemented using plain Python:\n",
    "```python\n",
    "class DenseLayerPlain:\n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.weights = [[0.5 for _ in range(output_size)] for _ in range(input_size)]\n",
    "        self.bias = [0.1 for _ in range(output_size)]\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        output = []\n",
    "        for i in range(len(self.weights[0])):\n",
    "            neuron_output = sum(inputs[j] * self.weights[j][i] for j in range(len(inputs))) + self.bias[i]\n",
    "            output.append(neuron_output)\n",
    "        return output\n",
    "\n",
    "# Example usage\n",
    "layer = DenseLayerPlain(3, 2)\n",
    "inputs = [1, 2, 3]\n",
    "output = layer.forward(inputs)\n",
    "print(output)\n",
    "```\n",
    "\n",
    "### 5. What is the “Hidden Size” of a Layer?\n",
    "The \"hidden size\" of a layer refers to the number of neurons in that hidden layer. It determines the capacity of the layer to learn and represent the data.\n",
    "\n",
    "### 6. What Does the `t` Method Do in PyTorch?\n",
    "The `t` method in PyTorch transposes a 2D tensor, swapping its rows and columns[^10^]. For example:\n",
    "```python\n",
    "import torch\n",
    "\n",
    "a = torch.tensor([[1, 2], [3, 4]])\n",
    "print(a.t())\n",
    "```\n",
    "\n",
    "### 7. Why is Matrix Multiplication Written in Plain Python Very Slow?\n",
    "Matrix multiplication in plain Python is slow because it lacks the optimizations and efficient memory management provided by libraries like NumPy, which are implemented in lower-level languages like C and Fortran.\n",
    "\n",
    "### 8. In `matmul`, Why is `ac == br`?\n",
    "In matrix multiplication, `ac == br` ensures that the number of columns in the first matrix (A) matches the number of rows in the second matrix (B), which is a requirement for the multiplication to be defined.\n",
    "\n",
    "### 9. Measuring Time Taken for a Single Cell to Execute in Jupyter Notebook\n",
    "You can measure the time taken for a single cell to execute using the `%%time` magic command:\n",
    "```python\n",
    "%%time\n",
    "# Your code here\n",
    "```\n",
    "\n",
    "### 10. What is Elementwise Arithmetic?\n",
    "Elementwise arithmetic refers to operations applied independently to each element of an array or tensor. For example, adding two arrays elementwise:\n",
    "```python\n",
    "a = np.array([1, 2, 3])\n",
    "b = np.array([4, 5, 6])\n",
    "c = a + b  # Elementwise addition\n",
    "```\n",
    "\n",
    "### 11. PyTorch Code to Test Whether Every Element of `a` is Greater Than the Corresponding Element of `b`\n",
    "```python\n",
    "import torch\n",
    "\n",
    "a = torch.tensor([1, 2, 3])\n",
    "b = torch.tensor([0, 2, 1])\n",
    "result = torch.gt(a, b)\n",
    "print(result)\n",
    "```\n",
    "\n",
    "### 12. What is a Rank-0 Tensor? How to Convert it to a Plain Python Data Type?\n",
    "A rank-0 tensor is a tensor with no dimensions, essentially a single scalar value. You can convert it to a plain Python data type using the `.item()` method:\n",
    "```python\n",
    "import torch\n",
    "\n",
    "a = torch.tensor(5)\n",
    "print(a.item())\n",
    "```\n",
    "\n",
    "### 13. How Does Elementwise Arithmetic Help Us Speed Up `matmul`?\n",
    "Elementwise arithmetic allows for parallel computation, which can be optimized by hardware accelerators like GPUs, speeding up matrix multiplication.\n",
    "\n",
    "### 14. Broadcasting Rules\n",
    "Broadcasting rules allow NumPy and PyTorch to perform elementwise operations on arrays of different shapes by automatically expanding their dimensions. The rules are:\n",
    "1. If the arrays have different ranks, prepend the shape of the smaller-rank array with ones.\n",
    "2. Compare the shapes elementwise, starting from the last dimension. Two dimensions are compatible if they are equal or one of them is one.\n",
    "\n",
    "### 15. What is `expand_as`? Example of Matching Results of Broadcasting\n",
    "The `expand_as` method in PyTorch expands a tensor to the same size as another tensor. Example:\n",
    "```python\n",
    "import torch\n",
    "\n",
    "a = torch.tensor([1, 2, 3])\n",
    "b = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "a_expanded = a.expand_as(b)\n",
    "print(a_expanded)\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ccf87c6",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
