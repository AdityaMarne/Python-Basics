{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a5d18ce",
   "metadata": {},
   "source": [
    "1. **How does unsqueeze help us to solve certain broadcasting problems?**\n",
    "   - The `unsqueeze` function in PyTorch adds a singleton dimension (a dimension with size 1) at a specified position in a tensor. This is useful for broadcasting because it allows tensors of different shapes to be compatible for element-wise operations. For example, if you have a tensor of shape \\([3, 4]\\) and you want to add a tensor of shape \\([4]\\), you can use `unsqueeze` to change the shape of the second tensor to \\([1, 4]\\), making them compatible for broadcasting.\n",
    "\n",
    "2. **How can we use indexing to do the same operation as unsqueeze?**\n",
    "   - You can achieve the same effect as `unsqueeze` using indexing by adding a new axis. For example, if you have a tensor `x` of shape \\([3, 4]\\), you can add a new dimension at position 0 using `x[None, :]`, which will change its shape to \\([1, 3, 4]\\). Similarly, `x[:, None, :]` will add a new dimension at position 1, resulting in a shape of \\([3, 1, 4]\\).\n",
    "\n",
    "3. **How do we show the actual contents of the memory used for a tensor?**\n",
    "   - In PyTorch, you can use the `.storage()` method to access the underlying storage of a tensor. For example, `tensor.storage()` will give you a view of the actual memory contents. Additionally, you can use `.data_ptr()` to get the pointer to the first element of the tensor's data.\n",
    "\n",
    "4. **When adding a vector of size 3 to a matrix of size 3Ã—3, are the elements of the vector added to each row or each column of the matrix?**\n",
    "   - When adding a vector of size 3 to a matrix of size \\(3 \\times 3\\), the elements of the vector are added to each row of the matrix. This is because broadcasting aligns the dimensions from the right, and the vector is treated as having shape \\([1, 3]\\), which is broadcasted to \\([3, 3]\\).\n",
    "\n",
    "5. **Do broadcasting and expand_as result in increased memory use? Why or why not?**\n",
    "   - Broadcasting does not result in increased memory use because it creates a view of the original tensor with the new shape, without actually copying the data. However, using `expand_as` can increase memory use if it creates a new tensor with the expanded shape, depending on the implementation.\n",
    "\n",
    "6. **Implement matmul using Einstein summation.**\n",
    "   - You can implement matrix multiplication using Einstein summation notation with the `einsum` function in PyTorch:\n",
    "     ```python\n",
    "     import torch\n",
    "     a = torch.randn(2, 3)\n",
    "     b = torch.randn(3, 4)\n",
    "     c = torch.einsum('ik,kj->ij', a, b)\n",
    "     ```\n",
    "\n",
    "7. **What does a repeated index letter represent on the lefthand side of einsum?**\n",
    "   - A repeated index letter on the left-hand side of `einsum` indicates summation over that index. For example, in the expression `einsum('ik,kj->ij', a, b)`, the index `k` is repeated, indicating that the summation is performed over the `k` dimension.\n",
    "\n",
    "8. **What are the three rules of Einstein summation notation? Why?**\n",
    "   - The three rules of Einstein summation notation are:\n",
    "     1. **Repeated indices are summed over**: Any index that appears twice in a single term is summed over.\n",
    "     2. **Free indices are not summed**: Indices that appear only once in a term are not summed and appear in the output tensor.\n",
    "     3. **Implicit summation**: The summation is implied and does not need to be explicitly written.\n",
    "   - These rules simplify tensor operations by reducing the need for explicit loops and making the notation more concise.\n",
    "\n",
    "9. **What are the forward pass and backward pass of a neural network?**\n",
    "   - The **forward pass** is the process of passing input data through the network to obtain the output predictions. During this pass, activations are calculated for each layer.\n",
    "   - The **backward pass** (or backpropagation) is the process of calculating gradients of the loss function with respect to each weight by applying the chain rule. These gradients are then used to update the weights during training.\n",
    "\n",
    "10. **Why do we need to store some of the activations calculated for intermediate layers in the forward pass?**\n",
    "    - Storing activations for intermediate layers is necessary because they are required during the backward pass to compute gradients. Without these stored activations, it would be impossible to calculate the gradients needed for updating the weights.\n",
    "\n",
    "11. **What is the downside of having activations with a standard deviation too far away from 1?**\n",
    "    - If the activations have a standard deviation too far away from 1, it can lead to issues such as vanishing or exploding gradients. This makes training difficult because the gradients can become too small to make meaningful updates or too large, causing instability.\n",
    "\n",
    "12. **How can weight initialization help avoid this problem?**\n",
    "    - Proper weight initialization can help ensure that the activations have a standard deviation close to 1. Techniques like Xavier (Glorot) initialization or He initialization are designed to keep the variance of activations and gradients stable across layers, which helps in avoiding vanishing or exploding gradients."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42f7a0d",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
